COURSE 2: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization
WEEK 2: Optimization Algorithms

+ Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
+ Use random minibatches to accelerate convergence and improve optimization
+ Describe the benefits of learning rate decay and apply it to your optimization